\documentclass[english]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tabless
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{csquotes}



% Bibliography
\usepackage[%
  backend=biber,%
  backref=false,%
  giveninits=true,%
  autocite=inline,%
  sorting=none,% in order of occurence. Other option: nyt (name year title)
  sortcites=true,%
  mincitenames=1,%
  maxcitenames=2,%
  maxbibnames=10,%
  doi=false,%
  isbn=false,%
  url=false,%
  natbib=true,
]{biblatex}

\addbibresource{neurips_2020.bib}

\title{Training Dense Object Nets: A Novel Approach}

\author{%
  Kanishk Navale, Ralf Gulde, Marc Tuscher, Oliver Riedel
}

\begin{document}



\maketitle

\begin{abstract}
  We present a novel framework for mining dense visual object descriptors produced by Dense Object Nets (DON) without 
  explicitly training DON. DON's dense visual object descriptors are robust to changes in viewpoint and configuration.
  However, training DON requires image pairs with correspondence mapping, which can be computationally expensive and 
  limit the dimensionality and robustness of the descriptors, limiting object generalization. 
  To overcome this, we propose a synthetic augmentation data generation procedure and a novel deep 
  learning architecture that produces denser visual descriptors while consuming fewer computational resources. 
  Furthermore, our framework does not require image-pair correspondence mapping and demonstrates its one of the applications 
  as a robot-grasping pipeline. Experiments show that our approach produces descriptors as robust as DON.
\end{abstract}

\section{Introduction}
\input{chapters/introduction.tex}

{\small \printbibliography}

\end{document}
