\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tabless
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{multirow}

% Bibliography
\usepackage[%
    backend=biber,%
    backref=false,%
    giveninits=true,%
    autocite=inline,%
    sorting=none,%
    sortcites=true,%
    mincitenames=1,%
    maxcitenames=2,%
    maxbibnames=10,%
    doi=true,%
    isbn=false,%
    url=false,%
    natbib=false,
]{biblatex}

\renewcommand*{\bibfont}{\small}

\addbibresource{../discography.bib}


\begin{document}

\title{Training Dense Object Nets: A Novel Approach}

\author{\IEEEauthorblockN{1\textsuperscript{st} Kanishk Navale}
    \IEEEauthorblockA{\textit{Sereact GmbH.} \\
        \textit{}
        Stuttgart, Germany \\
        kanishk.navale@sereact.ai}
    \and
    \IEEEauthorblockN{2\textsuperscript{nd} Ralf Gulde}
    \IEEEauthorblockA{\textit{Sereact GmbH.} \\
        \textit{}
        Stuttgart, Germany \\
        ralf.gulde@sereact.ai}
    \and
    \IEEEauthorblockN{3\textsuperscript{rd} Marc Tuscher}
    \IEEEauthorblockA{\textit{Sereact GmbH.} \\
        \textit{}
        Stuttgart, Germany \\
        marc.tuscher@sereact.ai}
    \and
    \IEEEauthorblockN{4\textsuperscript{th} Oliver Riedel}
    \IEEEauthorblockA{\textit{ISW - Universit√§t Stuttgart} \\
        \textit{}
        Stuttgart, Germany \\
        oliver.riedel@isw.uni-stuttgart.de}
}

\maketitle

\begin{abstract}
    Our work proposes a novel framework that addresses the computational limitations associated with training Dense Object Nets (DON)
    while achieving robust and dense visual object descriptors. DON's descriptors are known for their robustness to
    viewpoint and configuration changes, but training these requires image pairs with computationally expensive correspondence mapping.
    This limitation hampers dimensionality and robustness, thereby restricting object generalization.
    To overcome this, we introduce data generation procedure using synthetic augmentation and a novel deep learning architecture
    that produces denser visual descriptors with reduced computational demands. Notably, our framework eliminates the need for
    image pair correspondence mapping and showcases its application in a robotic grasping pipeline.
    Experimental results demonstrate that our approach yields descriptors as robust as those generated by DON.
\end{abstract}

\begin{IEEEkeywords}
    Dense Object Nets, robot grasping, generalized object representation, reduced computation costs.
\end{IEEEkeywords}

\section{Introduction}
\input{../chapters/introduction.tex}

\section{Related Works}
\input{../chapters/related.tex}

\section{Methodology}
\input{../chapters/methodology.tex}

\section{Experiments \& Results}
\input{../chapters/results.tex}

\section{Conclusion}
\input{../chapters/conclusion.tex}

\printbibliography

\end{document}
