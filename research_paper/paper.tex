\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2023}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tabless
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{csquotes}




% Bibliography
\usepackage[%
  backend=biber,%
  backref=false,%
  giveninits=true,%
  autocite=inline,%
  sorting=none,% in order of occurence. Other option: nyt (name year title)
  sortcites=true,%
  mincitenames=1,%
  maxcitenames=2,%
  maxbibnames=10,%
  doi=true,%
  isbn=false,%
  url=false,%
  natbib=false,
]{biblatex}

\renewcommand*{\bibfont}{\small}
\addbibresource{neurips_2020.bib}

\title{Training Dense Object Nets: A Novel Approach}

\author{%
  Kanishk Navale\textsuperscript{1}\thanks{for correspondence: kanishk.navale@sereact.ai}, \quad
  Ralf Gulde\textsuperscript{1, 2}, \quad
  Marc Tuscher\textsuperscript{1}, \quad
  Oliver Riedel \textsuperscript{2}\\
  \textsuperscript{1}Sereact GmbH, Stuttgart, Germany \quad
  \textsuperscript{2}ISW, Universit√§t Stuttgart, Stuttgart, Germany\\
}

\begin{document}



\maketitle

\begin{abstract}
  Our work proposes a novel framework that addresses the computational resource limitations associated with training Dense Object Nets (DON)
  while achieving robust and dense visual object descriptors. DON's descriptors are known for their robustness to
  viewpoint and configuration changes, but training them requires computationally expensive image pairs with correspondence mapping.
  This limitation hampers dimensionality and robustness, thereby restricting object generalization.
  To overcome this, we introduce a synthetic augmentation data generation procedure and a novel deep learning architecture
  that produces denser visual descriptors with reduced computational demands. Notably, our framework eliminates the need for
  image-pair correspondence mapping and showcases its application in a robot-grasping pipeline.
  Experimental results demonstrate that our approach yields descriptors as robust as those generated by DON.
\end{abstract}

\section{Introduction}
\input{chapters/introduction.tex}

\section{Related Work}
\input{chapters/related.tex}

\section{Methodology}
\input{chapters/methodology.tex}

\section{Experiments \& Results}
\input{chapters/results.tex}

\section{Conclusion}
\input{chapters/conclusion.tex}

\section{Future Work}
\input{chapters/future.tex}

\printbibliography

\end{document}
