The objectives of long-standing robotics and robotic manipulation are to create a general-purpose robot capable of carrying out practical activities like Chappie~\cite{blomkamp2015chappie} or C-3PO~\cite{lucas1977star}. While advancements have been made recently in adjacent domains, achieving this goal remains a work in progress. For instance, AlphaGo~\cite{silver2018general},
a game-playing artificial intelligence system trained entirely on self-play, defeated the world's best human Go player at the time. Subsequently, \citeauthor{silver2016mastering}~\cite{silver2016mastering}
developed artificial intelligence algorithms that mastered the game of chess, Go, World of Warcraft~\cite{entertainment2013world}, and Shogi,
surpassing human playing expertise. Most of these algorithms learn directly from visual data, such as gameplay recordings or online video streams, emphasizing the importance of visual data in AI. Meanwhile, the launch of AlexNet~\cite{krizhevsky2017imagenet} in 2012 transformed the field of computer vision. Other visual tasks, such as semantic segmentation~\cite{long2015fully}, object identification and recognition~\cite{he2017mask}, and human pose estimation~\cite{guler2018densepose}, have also witnessed significant gains in recent years. Significant breakthroughs have been made in robotics, ranging from self-driving cars to humanoid robots capable of performing complex tasks using cameras and other vision sensors. Despite these advancements, the most frequently used robotic manipulation systems have evolved slightly in the previous 30 years. Typical auto-factory robots continue to perform repetitive operations such as welding and painting, following a pre-programmed course with no feedback from the surroundings. If we want to increase the utility of our robots, we must move away from highly controlled settings and robots that perform repetitive actions with little feedback or adaptability capabilities. Liberating ourselves from these constraints of controlled settings-based manufacturing would allow us to enter new markets, as witnessed by the proliferation of firms~\cite{sereact} competing in the logistics domain.

The ideal object representation for robot grasping and manipulation tasks remains to be engineered today.
Existing representations may not be suitable for complex tasks due to limited capabilities of understanding an object's geometrical and structural information.
In \citeyear{florence2018dense}, \citeauthor{florence2018dense}~\cite{florence2018dense} introduced a novel visual
object representation to the robotics community,  terming it ``dense visual object descriptors''. DON, an artificial intelligence
framework proposed by
\Citeauthor{florence2018dense}~\cite{florence2018dense} produces dense visual object descriptors. In detail, the DON converts every pixel in the
image ($I[u, v] \in \mathbb{R}^3$) to a higher dimensional embedding ($I_D[u, v] \in \mathbb{R}^D$) such that $D \in \mathbb{N}^+$ consuming
image-pair correspondences as input yielding pixelwise embeddings
which are nothing but dense local descriptors.
The dense visual object descriptor generalizes an object up to a certain extent and has been recently
applied to rope manipulation \cite{rope-manipulation},
block manipulation \cite{block-manipulation}, robot control \cite{florence2019self}, fabric manipulation \cite{fabric-manipulation} and
robot grasp pose estimation \parencites{kupcsik2021supervised}{adrian2022efficient}. \citeauthor{adrian2022efficient}~\cite{adrian2022efficient}
further demonstrated that DON can be trained on synthetic data and still generalize to real-world objects. Furthermore, \citeauthor{adrian2022efficient}~\cite{adrian2022efficient} demonstrated that
the quality of descriptors produced by the DON framework depends on the higher or longer embedding dimension. We tried training the DON on a computation
device equipped with NVIDIA RTX A6000 GPU with 48GB VRAM. However, we could not train the DON to produce a higher embedding dimension due to the limited VRAM.
The DON framework is computationally expensive, as shown in Table~\ref{table:don_gpu_bechmark}, and limits the user to generalize objects to a certain extent making it
difficult to use as a robot grasping pipeline in real-world logistics and warehouse automation scenarios.

\begin{table}[htb]
    \caption{Benchmark of DON framework trained on GPU with 48GB VRAM with 128 image-pair correspondences, batch size of 1 and ``Pixelwise NTXENT Loss''~\cite{adrian2022efficient} as a loss function.}
    \label{table:don_gpu_bechmark}
    \centering
    \begin{tabular}{lllll}
        \toprule
        \multicolumn{5}{c}{GPU VRAM consumption to train DON}   \\
        \midrule
        Descriptor Dimension & 3     & 8      & 16     & 32     \\
        VRAM Usage (GB)      & 9.377 & 13.717 & 20.479 & 30.067 \\
        \bottomrule
    \end{tabular}
\end{table}

To overcome the computation resource limitation to produce denser visual object descriptors, we propose a novel framework to
train and extract dense visual object descriptors produced by DON, which is computationally efficient.