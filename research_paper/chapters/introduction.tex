Creating a general-purpose robotic system capable of performing practical tasks, such as those portrayed
in movies like Chappie~\cite{blomkamp2015chappie} or C-3PO~\cite{lucas1977star}, is a significant challenge in robotics.
While advancements have been made in related domains, achieving this goal remains an ongoing endeavour.
Recent artificial intelligence (AI) breakthroughs, particularly in deep learning, have demonstrated remarkable capabilities.
For instance, AlphaGo~\cite{silver2018general}, an AI system trained entirely on self-play, defeated the world's best human
Go player at the time. Subsequent algorithms, such as those developed by \citeauthor{silver2016mastering}~\cite{silver2016mastering},
have mastered complex games like chess, Go, World of Warcraft~\cite{entertainment2013world},
and Shogi, surpassing human expertise. These achievements underscore the importance of visual data in
deep learning, as these algorithms learn directly from visual inputs like gameplay recordings or online video streams.
Additionally, the introduction of AlexNet~\cite{krizhevsky2017imagenet} in 2012 has revolutionized computer vision,
leading to significant progress in tasks like semantic segmentation~\cite{long2015fully}, object identification
and recognition~\cite{he2017mask}, and human pose estimation~\cite{guler2018densepose}.

Significant strides have also been made in robotics, including developing self-driving cars and humanoid robots capable
of complex tasks using cameras and vision sensors. Integration of AI models, such as ChatGPT~\cite{openai2023gpt4} and
PaLM~\cite{palm}, has further enhanced the capabilities of robots. ChatGPT, developed by OpenAI, improves
human-robot interactions by generating coherent and contextually relevant responses, enabling robots to engage
in meaningful conversations and provide informative answers. Palm-E, developed by Stanford University, combines
computer vision and deep reinforcement learning, enabling robots to perceive and manipulate objects in their environment.
These AI models hold significant potential to enhance the dexterity and adaptability of robots, opening up new
possibilities across various industries.

However, developing and deploying these sophisticated AI models, particularly Large Language Models (LLMs)
like ChatGPT and Palm-E, present their own challenges. LLMs' sheer size and complexity demand vast computational resources,
raising concerns about excessive energy consumption and environmental impact. Researchers such as \Citeauthor{bender2021dangers}~\cite{bender2021dangers}
and \citeauthor{strubell2019energy}~\cite{strubell2019energy} have raised alarms about the substantial carbon emissions associated with training and operating
LLMs at scale, as well as the exacerbation
of the digital divide due to the resource-intensive nature of LLMs. Responsible resource allocation and addressing the ethical implications
of prioritizing computational power for LLMs are crucial for balancing technological progress and sustainability.

In robotics, while typical industrial robots perform repetitive operations based on pre-programmed instructions,
finding the ideal object representation for grasping and manipulation tasks remains unanswered.
Existing representations may be unable to understand an object's geometrical and structural information, rendering them unsuitable for complex tasks
In recent work, \citeauthor{florence2018dense}~\cite{florence2018dense} introduced a novel visual object representation termed "dense visual object descriptors" to the robotics community.
This representation, generated by the Dense Object Nets (DON) framework, converts each pixel in an image ($I[u, v] \in \mathbb{R}^3$)
into a higher-dimensional embedding ($I_D[u, v] \in \mathbb{R}^D$) such that $D \in \mathbb{N}^+$,
using image-pair correspondences as input. These dense visual object descriptors provide a generalized representation of objects to a certain extent.


The DON framework has shown promise in various domains, including rope manipulation~\cite{rope-manipulation},
block manipulation~\cite{block-manipulation}, robot control~\cite{florence2019self}, fabric manipulation~\cite{fabric-manipulation},
and robot grasp pose estimation~\parencites{kupcsik2021supervised}{adrian2022efficient}. \citeauthor{adrian2022efficient}~\cite{adrian2022efficient}
demonstrated that DON can be trained on
synthetic data and still generalize well to real-world objects. Furthermore, they highlighted the importance of the
dimensionality of the embedding in determining the quality of the descriptors produced by the DON framework.


In this paper, we address the challenges posed by the computationally intensive nature of AI and propose a new framework for training DON in a computationally efficient manner.
Our approach aims to contribute to developing sustainable and efficient AI solutions for robotics, bridging the gap between technological progress and environmental responsibility.