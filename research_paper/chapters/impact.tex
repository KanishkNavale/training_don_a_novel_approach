Today's Large Language Models (LLMs) consume vast computational resources, posing significant concerns for the future.
LLMs' sheer size and complexity require massive computing power, leading to excessive energy consumption and environmental harm.
Researchers and experts have raised alarm about these issues.
As \Citeauthor{bender2021dangers}~\cite{bender2021dangers} highlighted, the environmental impact of training and operating LLMs at scale is substantial, contributing to increased carbon emissions.
Furthermore, in their paper, \citeauthor{strubell2019energy}~\cite{strubell2019energy} emphasized how the resource-intensive nature of LLMs exacerbates the digital divide,
limiting access to advanced AI technologies for underprivileged populations. The ethical implications of prioritizing computational power for LLMs over binding
domains have also been acknowledged by researchers such as \citeauthor{amodei2016concrete}~\cite{amodei2016concrete}, stressing the need for responsible resource allocation. Striking a balance between
echnological progress and sustainability is essential, as echoed by various experts in the field.