We present a novel framework for mining dense visual object descriptors without explicitly training DON.
By leveraging synthetic augmentation data generation and a novel deep learning architecture,
our approach produces robust and denser visual local descriptors while consuming up to 86.67\% lesser computational resources than the originally proposed framework.
Furthermore, it eliminates the additional task of computing a large number of image-pair correspondences.
We demonstrate the application of our framework as a robot-grasping pipeline in two methodologies,
one of which our framework demonstrates its capabilities to produce object-specific 6D poses for robot grasping.
