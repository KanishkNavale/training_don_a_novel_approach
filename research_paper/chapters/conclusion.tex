We present a novel framework for mining dense visual object descriptors without explicitly training DON.
By leveraging synthetic augmentation data generation and a novel deep learning architecture, our approach produces robust
and denser visual local descriptors
while consuming XX\% lesser computational resources than originally proposed framework.
Futhermore, it eliminates the additional computation of mapping large number of image-pair correspondence mapping.
We demonstrate the application of our framework as a robot-grasping pipeline in two methodolgies one of which our
framework demonstrates its capabilities to produce object specific 6D poses for robot grasping.