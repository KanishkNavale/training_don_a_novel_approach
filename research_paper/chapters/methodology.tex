\subsection{Dataset Engineering}

We have chosen the cap object for creating synthetic dataset as the cap mesh models are readily available in the ``Shapenet'' library~\cite{chang2015shapenet}
as it containes rich object information including textures. Furthermore, we choose 5 cap models from the Shapenet library and use
Blenderproc~\cite{blenderproc} to generate the synthetic dataset.
We only save each cap model's one RGB image, mask and depth in the synthetic scene. Addtionally, we employ synthetic augmentations as proposed in \cite{adrian2022efficient}
to synthetically spatially augment cap's position and rotation in an image using Torchvision~\cite{marcel2010torchvision} library. To generate camera poses for different viewpoints,
an augmented image-pair is sampled randomly and image-pair correspondences is computed as demonstrated in \cite{adrian2022efficient}
\footnote[1]{GitHub Link: \url{https://github.com/KanishkNavale/Mapping-Synthetic-Correspondences-in-an-Image-Pair}}as illustrated in the figure.
Using depth information we project the computed correspondences to camera frame and compute relative transformation between spatial augmented images
using Kabsch's Transformation~\cite{kabsch}.

\subsection{Framework \& Mining Strategy}

As a backbone, we employ ResNet-34 architecture \cite{resnet}.
We preserve the last convolution layer and remove the pooling and linear layers.
The backbone downsamples the RGB image $I_{RGB} \in \mathbb{R}^{H \times W \times 3}$
to dense features $I_d \in \mathbb{R}^{h \times w \times D}$
such that $ h \ll H, w \ll W \text{ and } D \in \mathbb{N}^+$.
We upsample the dense features from the identity layer
(being identity to the last convolution layer in the backbone) as illustrated in the Figure~\ref{fig:modified_dnn} in page~\pageref{fig:modified_dnn} as follows:
\begin{equation}
    f_U: I \in \mathbb{R}^{h \times w \times D} \rightarrow I_D \in \mathbb{R}^{H \times W \times D},
\end{equation}
the upsampled dense features substituting as dense visual local descriptors produced from the DON in otherwords
we extract or mine the representations from the backbone.
Similarly as in \cite{suwajanakorn2018discovery}, we stack spatial-probability regressing layer and
depth regressing layer on top of the identity layer to predict $N \in \mathbb{N}^+$ number of keypoint's spatial-probability as a continuous method follows:
\begin{equation}
    f_S: I_d \in \mathbb{R}^{h \times w \times D} \rightarrow I_s^N \in \mathbb{R}^{h \times w \times N},
\end{equation}
and depth as follows:
\begin{equation}
    f_D: I_d \in \mathbb{R}^{h \times w \times D} \rightarrow I_{\hat{d}} \in \mathbb{R}^{h \times w \times N}.
\end{equation}

We incorporate continuous sampling method $f_E$ from \parencites{florence2020dense}{suwajanakorn2018discovery}
to convert the upsampled predicted spatial-probability and depth of a keypoint to spatial-depth expectation as follows:
\begin{equation}
    f_E \circ g_E:[I_s, I_{\hat{d}}] \rightarrow [u, v, d]^T \in \mathbb{R}^3 \text{ , where }  g_E: I \in \mathbb{R}^{h \times w \times N} \rightarrow I \in \mathbb{R}^{H \times W \times N}.
\end{equation}
Furthermore, we train the framework in a twin architecture fashion as proposed in
\parencites{chen2020simple}{zbontar2021barlow}{florence2018dense}{florence2020dense}{kupcsik2021supervised}{adrian2022efficient}{hadjivelichkov2021fully}{nerf-Supervision}
on the KeypointNet task.

\begin{figure}[htb]
    \centering
    \includegraphics[scale=0.35]{images/arch.png}
    \caption{Illustration of novel framework designed to efficiently compute and seamlessly extract dense visual object descriptors.
        During inference we extract dense visual object descriptors directly from the network and ignore predicted spatial-depth expectation of the keypoints.}
    \label{fig:modified_dnn}
\end{figure}


\subsection{Loss Function Modifications}

For training, we directly adopt silhoutte consistency loss ($\mathcal{L}_{obj}$), variance loss ($\mathcal{L}_{var}$) and separation loss ($\mathcal{L}_{sep}$) functions from \cite{suwajanakorn2018discovery} to train the network on the keypoint prediction task.
However, we modify the multi-view consistent loss and relative pose estimation loss. In the case of multi-view consistency loss we
project the predicted spatial-depth expectation using camera intrinsics as follows:
\begin{equation}
    X_{cam} \in \mathbb{R}^{3 \times 1} = \mathcal{I}_{cam}^{-1}  \ [u, v, 1.0]^T \otimes d \text{ , where  } \ \mathcal{I}_{cam} \in \mathbb{R}^{3 \times 3} \text{ and }  u, v, d \in \mathbb{R}^+.
\end{equation}

Furthermore, we project the camera coordinates of the keypoints regressed on both images to the
world coordinates using camera transformation and compute Huber Loss~\cite{huber1992robust} represented as $\mathcal{H}$ in Equation~\ref{eqn:mvc} as multi-view consistency loss as follows:
\begin{equation}
    \label{eqn:mvc}
    \mathcal{L}_{mvc} \in \mathbb{R} = \mathcal{H}(\mathcal{T}_{C \rightarrow W}^A \hat{X}^A_{cam}, \mathcal{T}_{C \rightarrow W}^B \hat{X}^B_{cam}) \text{ , where  } \hat{X}_{cam}=[X_{cam}, 1.0]^T \in \mathbb{R}^{4 \times 1} ,
\end{equation}

this modification is geometrically more intuitive as all the keypoints projected from different camera viewpoints into world coordinates occupy the same value.
Addtionally, using Huber Loss creates smoother gradients to optimize the framework compared to the original implementation of
using Euclidean distance to measure the projected keypoint's world coordinates into another camera viewpoint's pixel coordinates.
In Equation~\ref{eqn:mvc} $SE(3) \in \mathbb{R}^{4 \times 4}$ is a ``Special Euclidean Group''~\cite{thurston2014three}.
We do not discard the relative transformation information to calculate the realative pose loss as suggested in \cite{suwajanakorn2018discovery}
and being influenced from \cite{zhao2020learning} we modified the relative pose loss as follows:
\begin{equation}
    \mathcal{L}_{pose} = \Vert log(\mathcal{T}_{truth}^{\dagger} \mathcal{T}_{pred}) \Vert \text{ , where  } \ log: SE(3) \rightarrow \mathfrak{se}(3) \text{ and } \mathcal{T}^{\dagger} = \begin{bmatrix}
        R^T & -R^T t \\
        0^T & 1
    \end{bmatrix} \in SE(3).
\end{equation}





